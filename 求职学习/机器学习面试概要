关联分析
    https://www.zhihu.com/search?type=content&q=%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90
    1.项与项集
    项，指我们分析数据中的一个对象；项集，就是若干项的项构成的集合，如集合{牛奶、麦片、糖}是一个3项集
    2.支持度 （项集频数）
    support(A)=count(A)/count(dataset)=P(A)  
    3.置信度
    就是条件概率 关联规则{A->B}的置信度较高
    4.提升度
    关联规则{A->B}中，提升度是指{A->B}的置信度，除以B的支持度
    5.频繁项集
    如果项集I的支持度满足预定义的最小支持度阈值，则I是频繁项集。
    6.最小支持度
    用户或专家定义的衡量支持度的一个阈值

关联分析包含2个过程：从数据集中寻找频繁项集，从频繁项集中生成关联规则。
Apriori
FP-Tree

分类问题：

概念
1. 准确率
2. 召回率 == 正阳率
	2.1 假阳率 == 误杀率
3. F1 Score
4. ROC 曲线 x轴假阳 y轴正阳率
5. P-R 曲线 
掌握程度 3


逻辑回归

	公式：
	优点: 解释性好 占内存小 收敛速度快 
	缺点: 不是特别准 无法处理数据不平衡现象【构造损失权重弥补】 无法处理线性不可分时【引入kernel trick】

	问题1：为什么 LR 用交叉熵损失而不是平方损失？
		一般和平方损失函数（最小二乘法）拿来比较，因为线性回归用的就是平方损失函数，原因就是平方损失函数加上sigmoid的函数将会是一个非凸的函数，不易求解，会得到局部解，用对数似然函数得到高阶连续可导凸函数，可以得到最优解。
	问题2：.逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
		结果无影响，，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一
	问题3：为什么我们还是会在训练的过程当中将高度相关的特征去掉？
		加强可解释性

朴素贝叶斯 【生成模型】
	公式
	优点：解释性好 占内存小
	缺点：不是特别准　需要大量样本　

	问题1：朴素贝叶斯“朴素”在哪里？
	在已知分类Y的条件下，各个特征变量取值是相互独立的
	问题2： 在估计条件概率P(X|Y)时出现概率为0的情况怎么办
	引入λ，当λ=1时称为拉普拉斯平滑
	问题3：朴素与半朴素贝叶斯分类器
	问题３：特征值为连续性变量时候，怎么办？
	假设服从正太分布，可通过正太分布概率公式计算

	scikit-learn中的Naive Bayes模块，这个模块中有三个训练模块：GaussianNB、MultinomialNB、BernoulliNB

【引】（常见的生成方法有LDA主题模型、朴素贝叶斯算法和隐式马尔科夫模型等，常见的判别方法有SVM、LR等），生成方法学习出的是生成模型，判别方法学习出的是判别模型。

https://www.zhihu.com/people/wei_jin
一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。
如果如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适。
如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。


词向量 常问

	0. Fasttext 默认参数

		dim 100 ---> 我改为300d 了 ；原因论文推荐300d
		lr 0.1
		ws 5	
		chargram -1
		wordgram 1 ----> 我改为2

		loss              # loss function {ns, hs, softmax, ova} [softmax]
   		bucket            # number of buckets [2000000]
    	thread            # number of threads [number of cpus] # 异步梯度下降
    	

	别的地方没动
	

    1.有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。
    
    	总的数据量 从ETL平台上拿到的是3600w多
    	训练集 正例（2w）负例 100w 【分词后 12w左右】

    2.Word2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？
    如何评估训练出来的词向量的好坏
    
    回答2.1：输入和输出都是词onehot，取隐藏层的向量表示。
    回答2.2：
    教科书中的定义：
    	Distributional Representation 们一般建立一个 词语-上下文矩阵 (Word-Context Matrix) M。这个矩阵 M 的每一行对应一个词语，每一列对应着一种上下文关系.

    	在 distributed representation 中，强调的重点是每个词并非采用离散的表征方式（例如 one-hot encoding），而是被表征为一个低维，稠密的向量。
    回答2.3：

    	Distributional Semantics 的假说认为一个词语的词义可以从它在语料库中的分布得到体现，也就是说通过观察一个词出现的上下文(context)，我们可以了解到这个词的语义。这应该也是每个 NLPer 都听说过的那句话：You shall know a word by the company it keeps   (Firth, 1957)


    3.Word2vec模型如何做到增量训练



    4.大致聊一下word2vec这个模型的细节，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）
    解释一下 hierarchical softmax 的流程(CBOW and Skip-gram)

    CBOW 多对一 词袋模型背景词预测中心词 
    Skip-gram 一对多 中心词预测背景词
    
    基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。

    	输入层 onehot 输出 onehot 有隐藏层
    
    基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树

    	首先说 二叉树的确有很多中，比如二叉搜索树，AVL树，二叉堆等等
    	霍夫曼树的特点是权值大的节点保证接近根节点（这里面词权重可以看作为 权值），而词分布还是一个长尾分布，这样就保证了，频数较高出现的词可以减少大量的计算
    
    基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）

    	梯度计算复杂度 log（n），相当于多个逻辑回归相乘；反传
    
    基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点

    	也是 6 ，略
    
    聊一下负采样模型优点（为什么使用负采样技术）

    	Skip-gram 一对多 中心词预测背景词；噪声词的选取选k个，（近似操作）
    
    如何对输入进行负采样（负采样的具体实施细节是什么）
    
    负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）

    	 Skip-gram 负采样 改变了softmax 为 是否是背景词而做的二分类

    	 CBOW softmax 对不在正确路径上节点做的二分类  
    
    CBOW和skip-gram相较而言，彼此相对适合哪些场景

    	cbow 快，skip-gram对于生僻词不好

    	cbow 为什么会对生僻词表示不好？ 生僻词的时候会把周围词加起来；会降低其表征力
    
    有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来

    	有 cos值效果比较好
    
    详细聊一下Glove细节，它是如何进行训练的？有什么优点？什么场景下适合使用？与Word2vec相比，有什么区别（比如损失函数）？
    
    详细聊一下Fasttext细节，每一层都代表了什么？它与Wod2vec的区别在哪里？什么情况下适合使用Fasttext这个模型？
    
    ELMO的原理是什么？以及它的两个阶段分别如何应用？（第一阶段如何预训练，第二阶段如何在下游任务使用）
    ELMO的损失函数是什么？它是一个双向语言模型吗？为什么？
    
    ELMO的优缺点分别是什么？为什么可以做到一词多义的效果？


【引】
用户数据（谁）
	DAU MAU 新增率 留存率
行为数据（做了什么）
	PV（Page View）页面访问量
	UV（Unique Visitor）独立的访问量
	转化率 访问时长 访问深度 弹出率
业务数据（结果怎么样）
	GMV Gross Merchandise Volume 的简称。只要是订单，不管消费者是否付款、卖家是否发货、是否退货，都可放进 GMV 
	ARPPU是 Average revenue per paying user 的缩写，代表每个付费用户的平均收入
	GMV ARPU 付费人数 付费率 付费频次



常见数据处理方法：
过采样 【少数类别增加样本】
1.朴素随机过采样
2.SMOTE 算法 (Synthetic Minority Oversampling Technique）
3. ADASYN 算法

欠采样 【反之】
1.朴素随机欠采样
2. EasyEnsemble
3. Tomek Link 算法
4.NearMiss 
	NearMiss-1
	NearMiss-2
	NearMiss-3
Tomek Link 和 NearMiss 的核心都是在于 度量距离
只不过删点策略不同
掌握程度 3

【引】
距离的几个性质
　1)非负性、同一性
　2)对称性
 3)直递性 （次可加性）
范数的几个性质
 1)正定性：
 2)正齐次性：
 3)次可加性（三角不等式）


1. 手写 卷积 池化 

2. TensorFlow和pytorch的区别
    
    区别 #1——动态及静态图形定义
    区别 #2—— 调试  tfdbg 的特殊工具
    区别 #3——可视化
    区别 #4——部署

代码量的问题 


mse 和 mae 的区别
这二者均可以作为衡量指标

在数理统计上学过，一组数mse最小化 实际上是是对平均数数期望的求解
而mae 是对中位数

若作为 损失函数的话 因为其导数的性质
mse l2 在 0点平滑；mae l1 变化剧烈 错过最优解
而对于异常点的话 l2 则会抖动明显 抗噪能力差
所以作为回归问题的损失函数上可采用 huber loss


bert 
1. masked 处理 不同于传统 language model 的bi -lstm
相当于完形填空
2. Transformer encoder 这样每个词位的词都可以无视方向和距离的直接把句子中的每个词都有机会encoding进来
也可并行化
3. multi-head attention block 多头机制
将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息。上图中Multi-Head Attention 就是将 Scaled Dot-Product Attention 过程做 H 次，再把输出合并起来


4. 位置编码机制


list 和 tuple 的区别
1. tuple 只可以增加 vs list 无所谓
   tuple 更安全 list 更灵活
2. tuple id不可变 list id可变
3. tuple 更小一些



batch normalization 和 layer normalization 区别
batch 对一组 样本做缩放 
layer 对一个 做缩放

layer 适合RNN 对于 dims 层进行缩放，每一个word 的特征量纲进行归一化
而如果用了 batch 1会导致 如果输入的是一句话的话，padding 操作无意义
2本身 batch 关联性不大，强行 batch会导致丢失差异信息

CNN chennel 不能归一化 ，不同chenel 代表不同纬度的特征信息 例如 红10 绿0 黄 10 ！= 红1 绿 0 黄 1
