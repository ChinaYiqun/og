

覆盖率 99% 额外召回103% 人工抽检准确率97.5%


-	停用词处理？
-	低质量文本过滤 【正常 色情 敏感器官 恐怖 】
-	分词 jieba分词 盘龙根雕 箱包皮革
-	新增未登录词词典 目的过拟合问题
-	华莱士炸鸡 荒岭路店 vs 荒岭之路 溟
-	预处理
-	1% 英文店名 0.5% 繁体店名 韩文 日语店名 翻译
知识点1 skip-gram和cbow优缺点？ - fatfish的回答 - 知乎
-	https://www.zhihu.com/question/68112508/answer/363112214生僻字
-	 分词算法这里好好看看
LR模型 模型降低权重


知识点2 不同模型在不同类别的分类上的能力是不一样的

知识点3 LR模型 NB模型 降低权重 【手足情 】 【手足护养】

padans 刷题
SQL 优化 临时表 嵌套查寻 子查寻

面试官您好 
时间问题说一下 ...
我是王逸群 本科和硕士都是东北大学软件工程专业的
明年毕业 在实验室的研究方向是 复杂网络算 数据挖掘的方向
在研究生期间也包括在参加过一些有关数据的比赛 在4月份-7月份也在美团实习

那么简单说说实习期间的一些工作 和之前的项目
POI point of interest 
的文本分类工作
...
背景
简单来说就是我点开美团应用或者微信小程序 在首页会有一些推荐的 以店名为主 一些item 以这种图+文的文本块给用户展示
这种文本块 
但是 有些文本的内容会有一些 色情内容 令人不适的 恶心的内容 恐怖内容，我们需要对其进行打压。
我的任务针对文去做一 文本分类的工作 把低质量的这些文本判别出来（前提是不要有过多这种误杀影响订单）

这个任务其实一开始对于我来说是一个挑战因为我之前没有深入接触过nlp的工作
但是分类的问题总的来说难度还不是特别大

主要流程
【文本预处理 分词 文本数据增强 数据不均衡处理】

【主要的难点是提高召回率 ；降低假阳性】
	探索性数据分析 


	文本长度分布 语言种类分布 词频分布
	中位数 7 
	平均数8 		import langid

正阳            假阳
没预测出来的阳   预测正确的阴
3.9 0.1
0.1 3600-0.1

2 0
2 3509

全量样本的标签未知 ，用召回指标看不合适 ，一般看的是对于之前的覆盖率，额外召回率，以及误杀数

1. 新增了预定义词表, 新增店名(命名实体识别) 对于分词来说很有帮助 例如 吉野家 【吉野，家】
2. 并没采用停用此表进行 词语的删去 因为一些店名包含的停用词有一定的含义而且删去了之后会影像含义 【不二炸鸡】


3.1分词词库 
预定义词表 jieba分词 分词算法 单向字典树 双向字典树)

3.2 数据增强 

	手足情愫spa护养
	手足护养
人工手动的


4.模型的选择这边选择的都是简单的模型，为什么不选bert模型或者基于transformer 呢？
	4.1 首先之前我没到这个组之前 组里面的人已经做了用bert进行分类的尝试了，但是假阳性很高，
	例如把很多鲜花店识别成为了低质量词语，因为花圈礼仪（这是一个真阳）。其实具体问题应该去具体分析
	 bert 不一定适用于所有情况。
	4.2 而且lr，以及NB的可解释行也是不错的。 可用于根据实际的结果进行字和词的降权处理。
	4.3 模型的选择
	这里面 fasttext 用的是 腾讯的预训练向量 ，训练的时候采用的是skip-gram ，这里为什么采用skip-gram呢，因为skip-gram在一些生僻字的

	总结来说就是CBOW模型中input是context（周围词）而output是中心词，训练过程中其实是在从output的loss学习周围词的信息也就是embedding，但是在中间层是average的，一共预测V(vocab size)次就够了。skipgram是用中心词预测周围词，预测的时候是一对word pair，等于对每一个中心词都有K个词作为output，对于一个词的预测有K次，所以能够更有效的从context中学习信息，但是总共预测K*V词。

	4.4 重要参数
		fasttext 有默认调参的选项，autotuneDuration=600
		但是查阅相关论文发现
		    lr                # learning rate [0.1]
		    dim               # size of word vectors [100]
		    ws                # size of the context window [5]
		    epoch             # number of epochs [5]

	4.5 模型的融合

		stacking 的方式 （一层权重）

	4.6 上线相关
	4.7 其他  1% 英文店名 0.5% 繁体店名 韩文 日语店名 翻译

所以按照文本的分类问题的话 整个我的流程是 先是探索性数据分析 文本的预处理 

选择合适的模型并进行调整参数（重点） 最后进行测试上线以及模型的迭代。

奥对了，这个问题的主要难度还是集中在了 降低这个假阳 也就是误杀率（不要把正常的内容识别成低质量）
之前所在部门用的是这个模板匹配法 比较粗糙，而bert 和 xlbert 有比较严重的过拟合也就误杀









